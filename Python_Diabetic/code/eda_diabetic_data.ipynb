{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diabetic analysis & modeling\n",
    "## 1. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/diabetic_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.replace(\"?\", np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.readmitted.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- There are 50 columns and more than 100k rows for the diabetic data\n",
    "- The target label is **readmitted** which is category with 3 labels: \n",
    "    - \"No\": No readmission for inpatient\n",
    "    - \">30\": patient was readmitted in 30 days or more\n",
    "    - \"<30\": patient was readmitted in less than 30 days\n",
    "- There are total 48 input variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop([\"encounter_id\",\"patient_nbr\"],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Missing values & Imputation\n",
    "#### 1.1.1. Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values = df.isnull().sum()\n",
    "missing_values = missing_values[missing_values > 0].sort_values(ascending=False)\n",
    "missing_pct = missing_values / len(df) * 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_pct.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "missing_pct.plot(kind='bar', ax=ax)\n",
    "for idx,vals in enumerate(missing_pct):\n",
    "    ax.text(idx, vals, f\"{vals:.2f}%\", ha='center', va='bottom')\n",
    "ax.set_title('Percentage of Missing Values by Column')\n",
    "ax.set_ylabel('Percentage (%)')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.medical_specialty.value_counts(normalize=True)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Out of 48 input variables, 9 variables have missing values\n",
    "- **Weight** is a numeric variable and has 97% missing value. Although it is important feature, but due to lots of missing values, we have to ignore this variable.\n",
    "- Similarly, we remove **max_glu_serum and A1Cresult** due to major missing values\n",
    "- Medical_Specialty containts information for the treatment procedure. It has about 49% missing values. According to suggestion, we will convert the missing value to None and apply OHE to this.\n",
    "- Similarly, we will apply the same for **payer_code** and **race**\n",
    "- diag1, diag2, diag3 will be combined and imputed in the later part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.2. Imputations\n",
    "- First remove **weight** column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop([\"weight\",\"max_glu_serum\",\"A1Cresult\"],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Next, impute **medical_specialty** with **None**\n",
    "- To reduce the dimension of this variable, we merge all categorical with less than 1% occurrence to the new variable called **Rest**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.medical_specialty.fillna(\"None\",inplace=True)\n",
    "small_specialty = df.medical_specialty.value_counts(normalize=True)*100\n",
    "df.medical_specialty = df.medical_specialty.replace(small_specialty.index[small_specialty<=1].values,\"Rest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Impute **payer_code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.payer_code.fillna(\"None\",inplace=True)\n",
    "small_pcode = df.payer_code.value_counts(normalize=True)*100\n",
    "df.payer_code = df.payer_code.replace(small_pcode.index[small_pcode<=1].values,\"Rest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Impute **race**\n",
    "- We do not merge any other race for this varialbe due to very small amount of missing value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.race.fillna(\"None\",inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Impute **diag1, diag2, diag3**: replace Nan value with **None**\n",
    "- For each diag category, retain the top 1% category value (~top 20 category), the rest convert to **Rest**\n",
    "- Create new diag column with following criteria:\n",
    "    - if there are values in diag_1, diag_2 and diag_3: get the value from diag_1 only\n",
    "    - if there are values in diag_2 and diag_3 only: get the value from diag_2 only\n",
    "- Keep only diag column and remove the rest    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.diag_1.fillna(\"None\",inplace=True)\n",
    "df.diag_2.fillna(\"None\",inplace=True)\n",
    "df.diag_3.fillna(\"None\",inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind1 = df.diag_1.value_counts(normalize=True)*100\n",
    "df.diag_1 = df.diag_1.replace(ind1.index[ind1<=1].values,\"Rest\")\n",
    "\n",
    "ind2 = df.diag_2.value_counts(normalize=True)*100\n",
    "df.diag_2 = df.diag_2.replace(ind2.index[ind2<=1].values,\"Rest\")\n",
    "\n",
    "ind3 = df.diag_3.value_counts(normalize=True)*100\n",
    "df.diag_3 = df.diag_3.replace(ind3.index[ind3<=1].values,\"Rest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_diag(row):\n",
    "    for col in ['diag_1', 'diag_2', 'diag_3']:\n",
    "        val = row[col]\n",
    "        if pd.notna(val) and val != 'Rest' and val != 'None':\n",
    "            return val\n",
    "    return 'Rest'  # If all are \"Rest\" or None\n",
    "\n",
    "# Apply to DataFrame\n",
    "df['diag'] = df.apply(pick_diag, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop([\"diag_1\",\"diag_2\",\"diag_3\"],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Convert **age** from category to numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_map = {\n",
    "    '[0-10)': 5, '[10-20)': 15, '[20-30)': 25, '[30-40)': 35,\n",
    "    '[40-50)': 45, '[50-60)': 55, '[60-70)': 65, '[70-80)': 75,\n",
    "    '[80-90)': 85, '[90-100)': 95\n",
    "}\n",
    "df['age_mid'] = df['age'].map(age_map)\n",
    "df = df.drop([\"age\"],axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Catogorical and Numerical data analysis\n",
    "- Beside the object type categorical variables, there are 3 columns having categorical values in numeric format that need to be treated as categorical variables, they are **admission_type_id, discharge_disposition_id, admission_source_id**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = df.select_dtypes(\"object\").columns\n",
    "df_cat = pd.concat([df[cat_cols],df[[\"admission_type_id\",\"discharge_disposition_id\",\"admission_source_id\"]]],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_num = df.drop(df_cat.columns,axis=1)\n",
    "\n",
    "df_cat = df_cat.drop([\"readmitted\"],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.1. Categorical distribution plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_categorical_grid(df, cat_cols, target_col='readmitted', n_rows=11, n_cols=3):\n",
    "    # Set up figure\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(n_cols * 5, n_rows * 4))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    # Define color mapping\n",
    "    expected_order = [\"NO\", \"<30\", \">30\"]\n",
    "    colors = {\"NO\": \"green\", \"<30\": \"blue\", \">30\": \"red\"}\n",
    "\n",
    "    for i, col in enumerate(cat_cols):\n",
    "        ax = axes[i]\n",
    "        \n",
    "        # Build count DataFrame\n",
    "        count_df = (\n",
    "            df.groupby([col, target_col])\n",
    "            .size()\n",
    "            .reset_index(name='count')\n",
    "            .pivot(index=col, columns=target_col, values='count')\n",
    "            .fillna(0)\n",
    "        )\n",
    "\n",
    "        # Sort by total count descending\n",
    "        count_df['total'] = count_df.sum(axis=1)\n",
    "        count_df = count_df.sort_values(by='total', ascending=False).drop(columns='total')\n",
    "\n",
    "        # Normalize to get proportions\n",
    "        total_count = count_df.values.sum()\n",
    "        prop_df = count_df / total_count * 100\n",
    "\n",
    "        # Reorder columns to expected order\n",
    "        available_order = [t for t in expected_order if t in prop_df.columns]\n",
    "        prop_df = prop_df[available_order]\n",
    "\n",
    "        # Plot\n",
    "        prop_df.plot(\n",
    "            kind='bar',\n",
    "            stacked=True,\n",
    "            ax=ax,\n",
    "            color=[colors[c] for c in available_order],\n",
    "            legend=False\n",
    "        )\n",
    "\n",
    "        ax.set_title(col)\n",
    "        ax.set_ylabel(\"Proportion (%)\")\n",
    "        ax.set_xlabel(\"\")\n",
    "        ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "    # Hide unused axes\n",
    "    for j in range(i + 1, len(axes)):\n",
    "        axes[j].set_visible(False)\n",
    "\n",
    "    # Shared legend\n",
    "    handles = [plt.Rectangle((0,0),1,1,color=colors[k]) for k in expected_order if k in df[target_col].unique()]\n",
    "    labels = [k for k in expected_order if k in df[target_col].unique()]\n",
    "    fig.legend(handles, labels, title=target_col, loc='upper center', ncol=3)\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "    plt.show()\n",
    "\n",
    "plot_categorical_grid(df, df_cat.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the categorical plots we can see that: \n",
    "- **readmitted**: is the target variables and was plotted against all other categorical input data. The distribution of NO readmittion covers majority followed by \">30\" days value\n",
    "- **race**: more than 70% of patients are caucasian, this having good pattern in comparison to target variable\n",
    "- Some good distributions that should be retained are: **gender, payer_code, medical_Specialty, metformin, glipizide, glyburide, piolitazone, rosiglitazone, insulin, change, diabetesMed, diag, admission_type_id, discharge_disposition_id, admission_source_id**.\n",
    "- **admission_type_id**: more than 50% are type 1, which is Emergency intake, while Urgent and Elective are highly ranked behind\n",
    "- **discharge_disposition_id**: about 60% of diabetic discharge are to home, while 13% are discharge to SNF or home with home health service. So we can see that majority discharge are to home\n",
    "- **admission_source_id**: similar to admission_type, nearly 60% admission sources are from 7: Emergency room, followed by 30% of 1:Physician Referral\n",
    "- The following categorical variables are suggested to be removed due to extreme skewness: **repalinize, nateglinide, chloropropamite, acetohesamide, tolbutamide, acarbose, miglitol, troglitazone, tolazamide, examide, citoglipton, gliburide_metoformin, glipizide_metoformin, gplipiride_glitopetazone, troglitazone, metorformin_piglitazone**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cat_keep = df[[\"race\",\"gender\",\"payer_code\",\"medical_specialty\",\"metformin\",\"glimepiride\",\"glipizide\",\"glyburide\",\"pioglitazone\",\n",
    "                  \"rosiglitazone\",\"insulin\",\"change\",\"diabetesMed\",\"diag\",\"admission_type_id\", \"discharge_disposition_id\", \"admission_source_id\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.2. Numerical data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = df_num.corr(method=\"pearson\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,12))\n",
    "sns.heatmap(corr, annot=True, fmt=\".2f\", cmap=\"coolwarm\", square=True, linewidths=0.5)\n",
    "plt.title(\"Correlation Matrix\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Cross correlation matrix does not exhibit high collinearity between numeric variables\n",
    "- Next we compute VIF to check if there is any significant collinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data['Feature'] = df_num.columns\n",
    "vif_data['VIF'] = [variance_inflation_factor(df_num.values, i) for i in range(df_num.shape[1])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vif_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The VIF analysis shows potential high collinearity between number_dianoses and age, however, the VIF values are in acceptable range. Hence we do not remove anything\n",
    "- We will proceed to One Hot Encoding and data partitioning\n",
    "### 1.3. One Hot Encoding categorical values\n",
    "- There are 33 original categorical variables\n",
    "- After EDA, we narrow down to 17 categorical variables\n",
    "- Now, we apply One Hot Encoding to 17 variables and we have 153 new variables\n",
    "- Merge this new OHE dataframe with numerical data to have X dataframe\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cat_keep = df_cat_keep.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cat_ohe = pd.get_dummies(df_cat_keep,drop_first=False,dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cat_ohe.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.concat([df_cat_ohe,df_num],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df[\"readmitted\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. Partition X and y to training and testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,train_size=0.8,random_state=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Modeling\n",
    "### 2.1. Logistic Regression\n",
    "#### Perform K-Fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=10,shuffle=True,random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    LogisticRegressionCV(\n",
    "        Cs=10,                     # Number or list of regularization values to try\n",
    "        cv=10,                      # Number of cross-validation folds\n",
    "        multi_class='multinomial',# For softmax-style multi-class\n",
    "        solver='lbfgs',           # Good for multiclass\n",
    "        max_iter=1000,\n",
    "        scoring='accuracy',\n",
    "        random_state=123\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pipeline.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best C chosen for each class\n",
    "model = pipeline.named_steps['logisticregressioncv']\n",
    "print(\"Best C per class:\", model.C_)\n",
    "\n",
    "# Accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(\"Test Accuracy:\", accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
